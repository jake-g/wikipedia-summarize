{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URL to summarizehttps://en.wikipedia.org/wiki/Cat\n",
      "-------------------------------------------------\n",
      "                'Top N Summary'\n",
      "-------------------------------------------------\n",
      "[104] [105] [106] [107]\n",
      "Diseases\n",
      "Cats can suffer from a wide range of health problems, including infectious diseases, parasites, injuries, and chronic disease. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary and possibly also the Iberian Peninsula . However, a new study suggests cats are a much greater menace than previously thought and feral cats kill several billion birds each year in the United States. This discovery, combined with genetic studies, suggests cats were probably domesticated in the Middle East, in the Fertile Crescent around the time of the development of agriculture and then they were brought to Cyprus and Egypt. [253]\n",
      "Depictions of cats in art\n",
      "Jar with Small Looped Handles and Feline Design, Chim√∫ c. 1100-1400.\n",
      "\n",
      "\n",
      "-------------------------------------------------\n",
      "             'Mean Scored' Summary\n",
      "-------------------------------------------------\n",
      "[104] [105] [106] [107]\n",
      "Diseases\n",
      "Cats can suffer from a wide range of health problems, including infectious diseases, parasites, injuries, and chronic disease. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary and possibly also the Iberian Peninsula . However, a new study suggests cats are a much greater menace than previously thought and feral cats kill several billion birds each year in the United States. This discovery, combined with genetic studies, suggests cats were probably domesticated in the Middle East, in the Fertile Crescent around the time of the development of agriculture and then they were brought to Cyprus and Egypt.\n",
      "\n",
      "\n",
      "-------------------------------------------------\n",
      "             'Keywords'\n",
      "-------------------------------------------------\n",
      "limited\n",
      "wetlands\n",
      "similarity\n",
      "hats\n",
      "comparatively\n",
      "four\n",
      "catch\n",
      "sleep\n",
      "controversial\n",
      "oldest\n",
      "cyprus\n",
      "aggression\n",
      "poorly\n",
      "feeding\n",
      "penile\n",
      "9,000\n",
      "wildcat\n",
      "cm2\n",
      "vermin\n",
      "risk\n",
      "every\n",
      "conception\n",
      "immunized\n",
      "disturb\n",
      "skills\n",
      "path\n",
      "chew\n",
      "estimates\n",
      "direct\n",
      "likely\n",
      "subordinate\n",
      "estimated\n",
      "even\n",
      "errors\n",
      "organisms\n",
      "contributed\n",
      "depictions\n",
      "new\n",
      "net\n",
      "increasing\n",
      "consisting\n",
      "lybica\n",
      "leyhausen\n",
      "herb\n",
      "never\n",
      "hundreds\n",
      "reported\n",
      "108\n",
      "109\n",
      "china\n",
      "active\n",
      "103\n",
      "100\n",
      "101\n",
      "106\n",
      "107\n",
      "104\n",
      "105\n",
      "dry\n",
      "substance\n",
      "forum\n",
      "items\n",
      "study\n",
      "vulnerability\n",
      "saint\n",
      "rogaine\n",
      "stressed\n",
      "complicated\n",
      "changes\n",
      "stray\n",
      "explained\n",
      "norse\n",
      "highly\n",
      "brought\n",
      "would\n",
      "achieve\n",
      "negative\n",
      "therefore\n",
      "strike\n",
      "survive\n",
      "type\n",
      "spaying\n",
      "females\n",
      "populations\n",
      "successful\n",
      "naive\n",
      "99\n",
      "98\n",
      "adult\n",
      "breaths\n",
      "excellent\n",
      "91\n",
      "90\n",
      "93\n",
      "92\n",
      "95\n",
      "94\n",
      "97\n",
      "must\n",
      "abundance\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import numpy\n",
    "import requests\n",
    "from boilerpipe.extract import Extractor\n",
    "\n",
    "def summarize(url=None, html=None, n=100, cluster_threshold=5, top_sentences=5):\n",
    "\n",
    "    # Adapted from \"The Automatic Creation of Literature Abstracts\" by H.P. Luhn\n",
    "    #\n",
    "    # Parameters:\n",
    "    # * n  - Number of words to consider\n",
    "    # * cluster_threshold - Distance between words to consider\n",
    "    # * top_sentences - Number of sentences to return for a \"top n\" summary\n",
    "            \n",
    "    # Begin - nested helper function\n",
    "    def score_sentences(sentences, important_words):\n",
    "        scores = []\n",
    "        sentence_idx = -1\n",
    "    \n",
    "        for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "    \n",
    "            sentence_idx += 1\n",
    "            word_idx = []\n",
    "    \n",
    "            # For each word in the word list...\n",
    "            for w in important_words:\n",
    "                try:\n",
    "                    # Compute an index for important words in each sentence\n",
    "    \n",
    "                    word_idx.append(s.index(w))\n",
    "                except ValueError, e: # w not in this particular sentence\n",
    "                    pass\n",
    "    \n",
    "            word_idx.sort()\n",
    "    \n",
    "            # It is possible that some sentences may not contain any important words\n",
    "            if len(word_idx)== 0: continue\n",
    "    \n",
    "            # Using the word index, compute clusters with a max distance threshold\n",
    "            # for any two consecutive words\n",
    "    \n",
    "            clusters = []\n",
    "            cluster = [word_idx[0]]\n",
    "            i = 1\n",
    "            while i < len(word_idx):\n",
    "                if word_idx[i] - word_idx[i - 1] < cluster_threshold:\n",
    "                    cluster.append(word_idx[i])\n",
    "                else:\n",
    "                    clusters.append(cluster[:])\n",
    "                    cluster = [word_idx[i]]\n",
    "                i += 1\n",
    "            clusters.append(cluster)\n",
    "    \n",
    "            # Score each cluster. The max score for any given cluster is the score \n",
    "            # for the sentence.\n",
    "    \n",
    "            max_cluster_score = 0\n",
    "            for c in clusters:\n",
    "                significant_words_in_cluster = len(c)\n",
    "                total_words_in_cluster = c[-1] - c[0] + 1\n",
    "                score = 1.0 * significant_words_in_cluster \\\n",
    "                    * significant_words_in_cluster / total_words_in_cluster\n",
    "    \n",
    "                if score > max_cluster_score:\n",
    "                    max_cluster_score = score\n",
    "    \n",
    "            scores.append((sentence_idx, score))\n",
    "    \n",
    "        return scores    \n",
    "    \n",
    "    # End - nested helper function\n",
    "    \n",
    "    extractor = Extractor(extractor='ArticleExtractor', url=url, html=html)\n",
    "\n",
    "    # It's entirely possible that this \"clean page\" will be a big mess. YMMV.\n",
    "    # The good news is that the summarize algorithm inherently accounts for handling\n",
    "    # a lot of this noise.\n",
    "\n",
    "    txt = extractor.getText()\n",
    "    \n",
    "    sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "\n",
    "    words = [w.lower() for sentence in normalized_sentences for w in\n",
    "             nltk.tokenize.word_tokenize(sentence)]\n",
    "\n",
    "    fdist = nltk.FreqDist(words)\n",
    "\n",
    "    top_n_words = [w[0] for w in fdist.items() \n",
    "            if w[0] not in nltk.corpus.stopwords.words('english')][:n]\n",
    "    \n",
    "    scored_sentences = score_sentences(normalized_sentences, top_n_words)\n",
    "\n",
    "    # Summarization Approach 1:\n",
    "    # Filter out nonsignificant sentences by using the average score plus a\n",
    "    # fraction of the std dev as a filter\n",
    "\n",
    "    avg = numpy.mean([s[1] for s in scored_sentences])\n",
    "    std = numpy.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences\n",
    "                   if score > avg + 0.5 * std]\n",
    "\n",
    "    # Summarization Approach 2:\n",
    "    # Another approach would be to return only the top N ranked sentences\n",
    "\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-top_sentences:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "\n",
    "    # Decorate the post object with summaries\n",
    "\n",
    "    return dict(keywords = top_n_words, top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],\n",
    "                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "# sample_url = 'https://en.wikipedia.org/wiki/Cat'\n",
    "sample_url = raw_input('Enter URL to summarize ')\n",
    "summary = summarize(url=sample_url)\n",
    "\n",
    "# Alternatively, you can pass in HTML if you have it. Sometimes this approach may be\n",
    "# necessary if you encounter mysterious urllib2.BadStatusLine errors. Here's how\n",
    "# that would work:\n",
    "\n",
    "# sample_html = requests.get(sample_url).text\n",
    "# summary = summarize(html=sample_html)\n",
    "\n",
    "print \"-------------------------------------------------\"\n",
    "print \"                'Top N Summary'\"\n",
    "print \"-------------------------------------------------\"\n",
    "print \" \".join(summary['top_n_summary'])\n",
    "print\n",
    "print\n",
    "print \"-------------------------------------------------\"\n",
    "print \"             'Mean Scored' Summary\"\n",
    "print \"-------------------------------------------------\"\n",
    "print \" \".join(summary['mean_scored_summary'])\n",
    "print\n",
    "print\n",
    "print \"-------------------------------------------------\"\n",
    "print \"             'Keywords'\"\n",
    "print \"-------------------------------------------------\"\n",
    "for word in summary['keywords']:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
